[
  {
    "title": "Small Language Models",
    "full_title": "A Comprehensive Survey of Small Language Models in the Era of Large Language Models",
    "author": "Fali Wang",
    "abstract": "Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness",
    "file": "slm.md",
    "date": "2025-01-24"
  },
  {
    "title": "LLM Unlearning with Quantization",
    "full_title": "Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge.",
    "author": "Z. Zhang",
    "abstract": "Quantization is a compression technique that reduces the precision of a model's weights and activations, typically represented as floating-point numbers, to lower bit-width formats such as 8-bit or 4-bit integers.",
    "file": "quant_unlearn.md",
    "date": "2025-01-10"
  },
]